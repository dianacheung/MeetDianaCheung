# Diana Cheung Portfolio

## About Me

Hello, I am a technical writer based in California, aiming to make complex technical content easy to understand and engaging for the target audience. I specialize in crafting technical content supporting the developer experience, such as conceptual explainers, how-to guides, tutorials, code samples, and library documentation. My areas of interest are AI tech and dev tools. I have experience programming in JavaScript and Python and collaborating remotely with cross-functional teams and stakeholders.

Let’s discuss how I can help with your technical documentation. Please reach out via [LinkedIn](https://www.linkedin.com/in/meetdianacheung/).

## Relevant Works

### How to create a Vale linter custom rule: Manually vs. AI-prompting approach

A step-by-step tutorial on creating a custom rule in the open-source Vale prose linter. The official documentation is targeted towards intermediate-level programmers. The tutorial helps technical writers with basic coding experience by breaking down the process and providing AI prompts for assistance. It is an experiment to incorporate AI prompts in technical documentation to assist end users and their AI assistants in customizing a dev tool.

[Read Here](https://medium.com/@meetdianacheung/how-to-create-a-vale-linter-custom-rule-031aa00e4100)

### Meta Llama 2 vs. OpenAI GPT-4: A Comparative Analysis of an Open-Source vs. Proprietary LLM

A technical comparison between large language models Llama 2 and GPT-4 on the dimensions of model releases and architectures, LLM benchmarks, access methods, and selection factors for project use. The article aims to inform software engineers building LLM-powered apps of the strengths and weaknesses of both models. It also provides clear guidelines for picking a model based on a project’s utilization and complexity requirements and specific use cases.

[Read Here](https://medium.com/@meetdianacheung/meta-llama-2-vs-openai-gpt-4-785589efe15e)

### Introducing the Emerging LLM Tech Stack: Components for LLM Retrieval Augmented Generation (RAG)

A conceptual guide explaining the various tech stack layers for implementing large language model Retrieval Augmented Generation (RAG): data, model, orchestration, and operational. The article targets software engineers building LLM-powered apps and lists available tools in each layer for kickstarting development.

[Read Here](https://medium.com/@meetdianacheung/introducing-the-emerging-llm-tech-stack-17ddc83ca640)

### Demystifying Generative AI: Introducing the Underlying Technologies & Models of Generative AI

A conceptual guide explaining the evolution of generative AI, including machine learning concepts, deep learning models, transformer architecture, and large language models. The article educates software engineers newer to artificial intelligence and machine learning, providing them with easy-to-understand fundamentals.

[Read Here](https://www.codesmith.io/blog/demystifying-generative-ai-introducing-the-underlying-technologies-models-of-generative-ai)

### How to handle multiple WebRTC peer connections in a single client

A quick how-to guide on handling multiple peer connections with WebRTC, an open-source library for establishing direct connections between 2 browser clients to transfer live video and audio streams. The official documentation and available tutorials only illustrate 1 WebRTC connection per client or hardcoded multiple connections. The piece details a practical solution with code examples on managing dynamic connections for other software engineers dealing with the same issue.

[Read Here](https://medium.com/@meetdianacheung/how-to-handle-multiple-webrtc-peer-connections-in-a-single-client-e316c452aad9)
